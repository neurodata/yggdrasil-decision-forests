{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dcbc36e",
   "metadata": {},
   "source": [
    "# MIGHT example on Wise-1 cancer data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f1c657",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Import libraries and local functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bf5a862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependency libararies\n",
    "import warnings\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# treeple functions\n",
    "from treeple.ensemble import HonestForestClassifier\n",
    "from treeple.tree import ObliqueDecisionTreeClassifier\n",
    "from treeple.stats import build_oob_forest\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e25a6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local function for processing cancer data files\n",
    "def get_X_y(f, root=\"./data/\", cohort=[], verbose=False):\n",
    "    df = pd.read_csv(root + f)\n",
    "    non_features = [\n",
    "        \"Run\",\n",
    "        \"Sample\",\n",
    "        \"Library\",\n",
    "        \"Cancer Status\",\n",
    "        \"Tumor type\",\n",
    "        \"Stage\",\n",
    "        \"Library volume (uL)\",\n",
    "        \"Library Volume\",\n",
    "        \"UIDs Used\",\n",
    "        \"Experiment\",\n",
    "        \"P7\",\n",
    "        \"P7 Primer\",\n",
    "        \"MAF\",\n",
    "    ]\n",
    "    sample_ids = df[\"Sample\"]\n",
    "    # if sample is contains \"Run\" column, remove it\n",
    "    for i, sample_id in enumerate(sample_ids):\n",
    "        if \".\" in sample_id:\n",
    "            sample_ids[i] = sample_id.split(\".\")[1]\n",
    "    target = \"Cancer Status\"\n",
    "    y = df[target]\n",
    "    # convert the labels to 0 and 1\n",
    "    y = y.replace(\"Healthy\", 0)\n",
    "    y = y.replace(\"Cancer\", 1)\n",
    "    # remove the non-feature columns if they exist\n",
    "    for col in non_features:\n",
    "        if col in df.columns:\n",
    "            df = df.drop(col, axis=1)\n",
    "    nan_cols = df.isnull().all(axis=0).to_numpy()\n",
    "    # drop the columns with all nan values\n",
    "    df = df.loc[:, ~nan_cols]\n",
    "    # if cohort is not None, filter the samples\n",
    "    if cohort is not None:\n",
    "        # filter the rows with cohort1 samples\n",
    "        X = df[sample_ids.isin(cohort)]\n",
    "        y = y[sample_ids.isin(cohort)]\n",
    "    else:\n",
    "        X = df\n",
    "    if \"Wise\" in f:\n",
    "        # replace nans with zero\n",
    "        X = X.fillna(0)\n",
    "    # impute the nan values with the mean of the column\n",
    "    X = X.fillna(X.mean(axis=0))\n",
    "    # check if there are nan values\n",
    "    # nan_rows = X.isnull().any(axis=1)\n",
    "    nan_cols = X.isnull().all(axis=0)\n",
    "    # remove the columns with all nan values\n",
    "    X = X.loc[:, ~nan_cols]\n",
    "    if verbose:\n",
    "        if nan_cols.sum() > 0:\n",
    "            print(f)\n",
    "            print(f\"nan_cols: {nan_cols.sum()}\")\n",
    "            print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "        else:\n",
    "            print(f)\n",
    "            print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "    # X = X.dropna()\n",
    "    # y = y.drop(nan_rows.index)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd3398e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Process the Wise-1 cancer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90d48c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTORY = \"./\"\n",
    "N_JOBS = 4\n",
    "N_EST = 100 # 100_000\n",
    "\n",
    "# collect sample data\n",
    "sample_list_file = DIRECTORY + \"AllSamples.MIGHT.Passed.samples.txt\"\n",
    "sample_list = pd.read_csv(sample_list_file, sep=\" \", header=None)\n",
    "sample_list.columns = [\"library\", \"sample_id\", \"cohort\"]\n",
    "\n",
    "# get the sample ids for specific cohorts\n",
    "cohort1 = sample_list[sample_list[\"cohort\"] == \"Cohort1\"][\"sample_id\"]\n",
    "cohort2 = sample_list[sample_list[\"cohort\"] == \"Cohort2\"][\"sample_id\"]\n",
    "PON = sample_list[sample_list[\"cohort\"] == \"PanelOfNormals\"][\"sample_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91a73cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the samples for cohort 1\n",
    "X, y = get_X_y(\"WiseCondorX.Wise-1.csv.gz\", root=DIRECTORY, cohort=cohort1)\n",
    "\n",
    "# Save the processed data to a CSV file for use in the Yggdrasil notebook\n",
    "processed_data = X.copy()\n",
    "processed_data[\"Cancer Status\"] = y\n",
    "processed_data.to_csv(\"processed_wise1_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c9ee0cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(352, 2523)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cca2a3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Run axis-aligned MIGHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ed384d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2514145409986668\n"
     ]
    }
   ],
   "source": [
    "est = HonestForestClassifier(\n",
    "    n_estimators=N_EST,\n",
    "    max_samples=1.6,\n",
    "    max_features=0.3,\n",
    "    bootstrap=True,\n",
    "    stratify=True,\n",
    "    n_jobs=N_JOBS,\n",
    "    random_state=23,\n",
    "    honest_prior=\"ignore\",\n",
    "    honest_method='prune',\n",
    ")\n",
    "\n",
    "# record start time\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "# obtain tree level posteriors\n",
    "fitted_est, tree_proba = build_oob_forest(est, X, y)\n",
    "\n",
    "# calculate fitting time\n",
    "end_time = time.perf_counter()\n",
    "time_dif = end_time-start_time\n",
    "\n",
    "print(time_dif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c7e19307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain forest level posteriors\n",
    "forest_proba = np.nanmean(tree_proba, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4b0cc2",
   "metadata": {},
   "source": [
    "# Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b559b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependency libararies\n",
    "import warnings\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# treeple functions\n",
    "from treeple.ensemble import HonestForestClassifier\n",
    "from treeple.tree import ObliqueDecisionTreeClassifier\n",
    "from treeple import ObliqueRandomForestClassifier\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from treeple.stats import build_oob_forest\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import ydf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import sys\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "random_seed = 42\n",
    "\n",
    "\n",
    "def benchmark_alg(n_vals, d_vals, classifier, repeats=7, random_state=42, log_file_path=None):  # NEW: pass path\n",
    "    \"\"\"\n",
    "    Benchmarks MIGHT's training & inference time across multiple (n, d) data sizes,\n",
    "    repeating each (n,d) `repeats` times to compute mean & std dev.\n",
    "\n",
    "    Returns a DataFrame with columns:\n",
    "      [\"n\", \"d\",\n",
    "       \"train_time_mean\", \"train_time_std\",\n",
    "       \"inference_time_mean\", \"inference_time_std\",\n",
    "       \"accuracy_mean\", \"accuracy_std\"]\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    results = []\n",
    "\n",
    "    for n in n_vals:\n",
    "        for d in d_vals:\n",
    "            train_times = []\n",
    "            inference_times = []\n",
    "            accuracies = []\n",
    "\n",
    "            for _ in range(repeats):\n",
    "                # 1) Generate random data\n",
    "                X = np.random.randn(n, d)\n",
    "\n",
    "                # TODO Important - Ariel: Testing whether a simple Fortran layout here would improve perf.\n",
    "                X = np.asfortranarray(X)\n",
    "\n",
    "                y = np.random.randint(2, size=n)\n",
    "\n",
    "                # 2) Split\n",
    "                X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    X, y, test_size=0.3, random_state=random_state, stratify=y\n",
    "                )\n",
    "\n",
    "                # 3) Convert to DataFrame. TODO Ariel - Disable this to avoid potentially switching back to C order\n",
    "                # df_train = pd.DataFrame(X_train)\n",
    "                df_train = X_train\n",
    "                # df_test  = pd.DataFrame(X_test)\n",
    "                df_test = X_test\n",
    "\n",
    "                # 4) Build classifier\n",
    "                # You could define this earlier, or globally, etc.\n",
    "                est = classifier\n",
    "\n",
    "                # 5) Training time\n",
    "                t0 = time.perf_counter()\n",
    "                fitted_est, tree_proba = build_oob_forest(est, df_train, y_train)\n",
    "                t1 = time.perf_counter()\n",
    "                train_times.append(t1 - t0)\n",
    "\n",
    "                # 6) Inference time\n",
    "                t2 = time.perf_counter()\n",
    "\n",
    "                # Get predicted probabilities directly from the fitted ensemble\n",
    "                y_proba_test = fitted_est.predict_proba(df_test)[:, 1]\n",
    "\n",
    "                # Convert probabilities to binary predictions\n",
    "                y_pred_test = (y_proba_test >= 0.5).astype(int)\n",
    "\n",
    "                # Measure inference time\n",
    "                t3 = time.perf_counter()\n",
    "                inference_times.append(t3 - t2)\n",
    "\n",
    "                # Compute accuracy\n",
    "                acc = accuracy_score(y_test, y_pred_test)\n",
    "                accuracies.append(acc)\n",
    "\n",
    "                # 7) Accuracy\n",
    "                acc = accuracy_score(y_test, y_pred_test)\n",
    "                accuracies.append(acc)\n",
    "\n",
    "            # Mean/std\n",
    "            train_time_mean = np.mean(train_times)\n",
    "            train_time_std  = np.std(train_times)\n",
    "            inf_time_mean   = np.mean(inference_times)\n",
    "            inf_time_std    = np.std(inference_times)\n",
    "            acc_mean        = np.mean(accuracies)\n",
    "            acc_std         = np.std(accuracies)\n",
    "\n",
    "            msg = (\n",
    "                f\"\\n(n={n}, d={d}):\\n\"\n",
    "                f\"  train_time mean={train_time_mean:.4f}s, std={train_time_std:.4f}\\n\"\n",
    "                f\"  inference_time mean={inf_time_mean:.4f}s, std={inf_time_std:.4f}\\n\"\n",
    "                f\"  accuracy mean={acc_mean:.3f}, std={acc_std:.3f}\"\n",
    "            )\n",
    "\n",
    "            # Print to screen\n",
    "            print(msg)\n",
    "\n",
    "            # Write to file right away, no buffering\n",
    "            if log_file_path:  # NEW\n",
    "                with open(log_file_path, \"a\") as f:  # open in append mode\n",
    "                    f.write(msg + \"\\n\")\n",
    "                    f.flush()  # force flush, ensures content is written\n",
    "\n",
    "            results.append({\n",
    "                \"n\": n,\n",
    "                \"d\": d,\n",
    "                \"train_time_mean\": train_time_mean,\n",
    "                \"train_time_std\": train_time_std,\n",
    "                \"inference_time_mean\": inf_time_mean,\n",
    "                \"inference_time_std\": inf_time_std,\n",
    "                \"accuracy_mean\": acc_mean,\n",
    "                \"accuracy_std\": acc_std,\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a2ba0d",
   "metadata": {},
   "source": [
    "## MIGHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bd04c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "honest_classifier = HonestForestClassifier(\n",
    "    n_estimators=1000,\n",
    "    max_samples=1.0, # Use all samples for bootstrapping. Use 1.0 instead of 1. 1 means literally 1 sample\n",
    "    max_features=128, # Keep this static\n",
    "    bootstrap=True,\n",
    "    stratify=True,\n",
    "    n_jobs=-1, # Use all resources\n",
    "    random_state=42,\n",
    "    honest_prior=\"ignore\",\n",
    "    honest_method='prune',\n",
    "    tree_estimator=ObliqueDecisionTreeClassifier(),\n",
    "    max_depth = None, # Fully grow trees\n",
    ")\n",
    "\n",
    "oblique_classifier = ObliqueRandomForestClassifier(\n",
    "    n_estimators=1000,\n",
    "    max_samples=1.0, # Use all samples for bootstrapping. Use 1.0 instead of 1. 1 means literally 1 sample\n",
    "    # max_features=128, # Keep this static\n",
    "    bootstrap=True,\n",
    "    n_jobs=-1, # Use all resources\n",
    "    random_state=42,\n",
    "    max_depth = None, # Fully grow trees\n",
    "    oob_score=False # Don't Inference OOB samples\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcccc70",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Honest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1834a27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_values = [128, 256,512,1024,2048,4096,8192, 16384, 32768, 65536, 131072]\n",
    "d_values = [128,256,512,1024,2048,4096,8192, 16384, 32768, 65536, 131072]\n",
    "\n",
    "# If you want a header first, you can do that outside:\n",
    "with open(\"might_benchmark_output.txt\", \"w\") as f:\n",
    "    f.write(\"=== MIGHT Benchmark Output ===\\n\")\n",
    "    f.flush()\n",
    "\n",
    "df_bench = benchmark_alg(n_values, d_values, classifier=honest_classifier, repeats=3,\n",
    "                            random_state=42, log_file_path=\"might_benchmark_output.txt\")\n",
    "\n",
    "# Final summary\n",
    "summary = \"\\n=== MIGHT Benchmark Results (Averaged Over Repeats) ===\\n\"\n",
    "print(summary)\n",
    "with open(\"might_benchmark_output.txt\", \"a\") as f:\n",
    "    f.write(summary + \"\\n\")\n",
    "    f.flush()\n",
    "\n",
    "print(df_bench)\n",
    "with open(\"might_benchmark_output.txt\", \"a\") as f:\n",
    "    f.write(df_bench.to_string(index=False) + \"\\n\")\n",
    "    f.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe992e07",
   "metadata": {},
   "source": [
    "### Oblique, non-Honest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab58511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(n=128, d=128):\n",
      "  train_time mean=1.8768s, std=0.0338\n",
      "  inference_time mean=0.0986s, std=0.0013\n",
      "  accuracy mean=0.444, std=0.032\n",
      "\n",
      "(n=128, d=256):\n",
      "  train_time mean=1.8829s, std=0.0079\n",
      "  inference_time mean=0.1003s, std=0.0013\n",
      "  accuracy mean=0.496, std=0.024\n",
      "\n",
      "(n=128, d=512):\n",
      "  train_time mean=1.9386s, std=0.0532\n",
      "  inference_time mean=0.1047s, std=0.0014\n",
      "  accuracy mean=0.573, std=0.024\n",
      "\n",
      "(n=128, d=1024):\n",
      "  train_time mean=2.0406s, std=0.0244\n",
      "  inference_time mean=0.0981s, std=0.0011\n",
      "  accuracy mean=0.581, std=0.053\n",
      "\n",
      "(n=128, d=2048):\n",
      "  train_time mean=2.2252s, std=0.0386\n",
      "  inference_time mean=0.1008s, std=0.0006\n",
      "  accuracy mean=0.479, std=0.079\n",
      "\n",
      "(n=128, d=4096):\n",
      "  train_time mean=3.7329s, std=0.0957\n",
      "  inference_time mean=0.1124s, std=0.0109\n",
      "  accuracy mean=0.453, std=0.044\n"
     ]
    }
   ],
   "source": [
    "n_values = [128, 256,512,1024,2048,4096,8192, 16384, 32768, 65536]#, 131072]\n",
    "d_values = [128,256,512,1024,2048,4096,8192, 16384, 32768, 65536]#, 131072]\n",
    "\n",
    "perf_logpath = \"../benchmark_results/oblique_treeple_benchmark_output.txt\"\n",
    "\n",
    "# If you want a header first, you can do that outside:\n",
    "with open(perf_logpath, \"w\") as f:\n",
    "    f.write(\"=== OBLIQUE MIGHT Benchmark Output ===\\n\")\n",
    "    f.flush()\n",
    "\n",
    "df_bench = benchmark_alg(n_values, d_values, classifier=oblique_classifier, repeats=3,\n",
    "                            random_state=42, log_file_path=perf_logpath)\n",
    "\n",
    "# Final summary\n",
    "summary = \"\\n=== MIGHT Benchmark Results (Averaged Over Repeats) ===\\n\"\n",
    "print(summary)\n",
    "with open(perf_logpath, \"a\") as f:\n",
    "    f.write(summary + \"\\n\")\n",
    "    f.flush()\n",
    "\n",
    "print(df_bench)\n",
    "with open(perf_logpath, \"a\") as f:\n",
    "    f.write(df_bench.to_string(index=False) + \"\\n\")\n",
    "    f.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deee5b41",
   "metadata": {},
   "source": [
    "## YDF ðŸŒ³"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ca4249",
   "metadata": {},
   "source": [
    "### Synthetic Scaling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64004d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "ydf_est = ydf.RandomForestLearner(\n",
    "                    label=\"label\",\n",
    "                    random_seed=random_seed,\n",
    "                    split_axis=\"SPARSE_OBLIQUE\",\n",
    "                    num_trees=1000,\n",
    "                    bootstrap_training_dataset=True,\n",
    "                    bootstrap_size_ratio=1.0,\n",
    "                    num_threads=96,\n",
    "                    max_depth=-1,\n",
    "\n",
    "                    sparse_oblique_projection_density_factor=128, # This should finally set n_nonzeros in proj matrix constant (in expectation)\n",
    "\n",
    "                    # sparse_oblique_max_num_projections = 128,\n",
    "                    # sparse_oblique_num_projections_exponent=0.0, # \"\"\" This should make max_num_projections always WIN \"\"\"\n",
    "\n",
    "                    # ''' Should skip computing OOB after training'''\n",
    "                    honest=False,\n",
    "                    compute_oob_performances=False, # Should skip computing OOB after training\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ace4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_values = [128, 256,512,1024,2048,4096,8192, 16384, 32768, 65536]#, 131072]\n",
    "p_values = [128,256,512,1024,2048,4096,8192, 16384, 32768, 65536]#, 131072]\n",
    "\n",
    "perf_logpath = \"../results/ydf_benchmark_output.txt\"\n",
    "\n",
    "# If you want a header first, you can do that outside:\n",
    "with open(perf_logpath, \"w\") as f:\n",
    "    f.write(\"=== YDF Benchmark Output ===\\n\")\n",
    "    f.flush()\n",
    "\n",
    "df_bench = benchmark_alg(n_values, p_values, repeats=3, classifier=ydf_est,\n",
    "                            random_state=42, log_file_path=perf_logpath)\n",
    "\n",
    "summary = \"\\n=== YDF Benchmark Results (Averaged Over Repeats) ===\\n\"\n",
    "print(summary)\n",
    "with open(perf_logpath, \"a\") as f:\n",
    "    f.write(summary + \"\\n\")\n",
    "    f.flush()\n",
    "\n",
    "print(df_bench)\n",
    "with open(perf_logpath, \"a\") as f:\n",
    "    f.write(df_bench.to_string(index=False) + \"\\n\")\n",
    "    f.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd31cc7",
   "metadata": {},
   "source": [
    "### Real MIGHT data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823465ba-b2e1-47e1-b3b4-297fa281b017",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "# 1) Imports and Basic Setup\n",
    "###############################################\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# This is the current Python package for Yggdrasil Decision Forests.\n",
    "import ydf\n",
    "\n",
    "###############################################\n",
    "# 2) Load the Exact Same Processed Data\n",
    "###############################################\n",
    "# In your MIGHT notebook, you saved something like \"processed_wise1_data.csv\".\n",
    "# Let's read that in so YDF sees the identical data.\n",
    "\n",
    "PROCESSED_DATA = \"processed_wise1_data.csv\"\n",
    "df = pd.read_csv(PROCESSED_DATA)\n",
    "\n",
    "# Suppose the label column is named \"Cancer Status\".\n",
    "LABEL_COL = \"Cancer Status\"\n",
    "\n",
    "print(\"DataFrame shape:\", df.shape)\n",
    "# print(\"Columns:\", df.columns.tolist())\n",
    "\n",
    "###############################################\n",
    "# 3) Verify We Have the Same Classification Task\n",
    "###############################################\n",
    "# Ensure that the label distribution matches what you see in MIGHT (e.g., y.value_counts()).\n",
    "\n",
    "num_rows, num_cols = df.shape\n",
    "if LABEL_COL not in df.columns:\n",
    "    raise ValueError(f\"Label column {LABEL_COL!r} not found in CSV columns!\")\n",
    "\n",
    "print(\"Number of samples (rows):\", num_rows)\n",
    "print(\"Number of columns:\", num_cols)\n",
    "print(f\"Label '{LABEL_COL}' distribution:\\n{df[LABEL_COL].value_counts()}\")\n",
    "\n",
    "# Optional: Print the first few columns to confirm\n",
    "print(\"Feature columns (excluding label):\")\n",
    "feature_cols = [c for c in df.columns if c != LABEL_COL]\n",
    "print(feature_cols[:10], \"...\" if len(feature_cols) > 10 else \"\")\n",
    "\n",
    "###############################################\n",
    "# 4) Train a YDF Random Forest\n",
    "###############################################\n",
    "# Provide the label name, and optionally set random_seed for reproducibility.\n",
    "\n",
    "rf_learner = ydf.RandomForestLearner(\n",
    "    label=LABEL_COL,\n",
    "    random_seed=42,  # ensures reproducible random sampling\n",
    "    num_trees=100,   # match MIGHT for fair comparison\n",
    "    max_depth=10     # or whatever matches your MIGHT hyperparams\n",
    ")\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "rf_model = rf_learner.train(df)\n",
    "end_time = time.perf_counter()\n",
    "\n",
    "train_time = end_time - start_time\n",
    "print(f\"\\n[INFO] Yggdrasil RF trained in {train_time:.2f} seconds.\")\n",
    "\n",
    "###############################################\n",
    "# 5) Evaluate / Inspect the Model\n",
    "###############################################\n",
    "# Evaluate on the same data (for quick demonstration).\n",
    "evaluation = rf_model.evaluate(df)\n",
    "print(\"\\nEvaluation metrics on the training set:\")\n",
    "print(evaluation)\n",
    "\n",
    "# If you want, you can also get predictions or partial-dependence analysis:\n",
    "predictions = rf_model.predict(df)\n",
    "print(\"\\nSample predictions (first 5 rows):\")\n",
    "print(predictions)\n",
    "\n",
    "###############################################\n",
    "# 6) Confirm Similarity with MIGHT\n",
    "###############################################\n",
    "# For a fair side-by-side timing:\n",
    "# - MIGHT uses the same \"processed_wise1_data.csv\".\n",
    "# - Both have 100 trees, same random seed if possible.\n",
    "# - Start/stop the timer at the exact training call.\n",
    "#\n",
    "# Now the only difference should be the algorithms/impl details themselves.\n",
    "###############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1c3c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import ydf  # Yggdrasil Decision Forests\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# 1) Read the same processed CSV as MIGHT\n",
    "CSV_FILE = \"processed_wise1_data.csv\"\n",
    "df = pd.read_csv(CSV_FILE)\n",
    "\n",
    "LABEL_COL = \"Cancer Status\"\n",
    "if LABEL_COL not in df.columns:\n",
    "    raise ValueError(f\"Missing label column {LABEL_COL!r} in CSV.\")\n",
    "\n",
    "# 2) Train/test split (hold-out)\n",
    "#    We'll separate 30% of the data for testing. Use the same random_state so you can replicate in MIGHT.\n",
    "X = df.drop(columns=[LABEL_COL])\n",
    "y = df[LABEL_COL]\n",
    "train_df, test_df = train_test_split(df, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "print(\"Train set shape:\", train_df.shape)\n",
    "print(\"Test set shape:\", test_df.shape)\n",
    "\n",
    "# 3) Build a YDF RandomForest with ~the same hyperparams as MIGHT\n",
    "#    e.g. 100 trees, random_seed=23 or 42, etc.\n",
    "rf_learner = ydf.RandomForestLearner(\n",
    "    label=LABEL_COL,\n",
    "    random_seed=42,\n",
    "    num_trees=100 #100000\n",
    ")\n",
    "\n",
    "# 4) Train on the HOLD-OUT train set\n",
    "start_time = time.perf_counter()\n",
    "rf_model = rf_learner.train(train_df)  # Only pass the train portion\n",
    "end_time = time.perf_counter()\n",
    "train_time = end_time - start_time\n",
    "print(f\"\\nYDF model trained in {train_time:.2f} seconds on {len(train_df)} examples.\")\n",
    "\n",
    "# 5) Evaluate on the test set using YDF's built-in evaluate()\n",
    "evaluation = rf_model.evaluate(test_df)\n",
    "print(\"\\n=== YDF Evaluate() on Test Set ===\")\n",
    "print(evaluation)\n",
    "\n",
    "#   By default, it prints metrics like \"accuracy\", \"confusion matrix\", \"ROC AUC\",\n",
    "#   etc. in a structured text output.\n",
    "\n",
    "# 6) (Optional) Compute scikit-learn metrics on the test set\n",
    "#    The YDF model's `.predict()` returns a DataFrame with probabilities for label=1 by default.\n",
    "preds = rf_model.predict(test_df)  # shape: (num_test_examples,) containing probabilities\n",
    "preds_np = preds#.to_numpy().ravel()\n",
    "\n",
    "# Convert probabilities -> predicted class using 0.5 threshold\n",
    "pred_labels = (preds_np >= 0.5).astype(int)\n",
    "\n",
    "test_y = test_df[LABEL_COL].values\n",
    "\n",
    "acc = accuracy_score(test_y, pred_labels)\n",
    "cm = confusion_matrix(test_y, pred_labels)\n",
    "cls_rpt = classification_report(test_y, pred_labels)\n",
    "\n",
    "print(\"\\n=== Scikit-learn metrics on Test Set ===\")\n",
    "print(f\"Accuracy: {acc:.3f}\")\n",
    "print(\"Confusion matrix:\\n\", cm)\n",
    "print(\"\\nClassification Report:\\n\", cls_rpt)\n",
    "\n",
    "# 7) Display sample predicted probabilities (first 10), to mimic the style you saw\n",
    "print(\"\\nSample predicted probabilities (first 10):\")\n",
    "print(preds_np[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
