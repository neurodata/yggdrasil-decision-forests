{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad69bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "YDF Log Parser - Notebook Interface\n",
    "Parse timing logs from YDF synthetic-data benchmarks and convert to CSV\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from typing import Dict, Any\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION & CONSTANTS\n",
    "# ============================================================================\n",
    "\n",
    "ORDER_EXACT = [\n",
    "    \"Selecting Bootstrapped Samples\",\n",
    "    \"Initialization of FindBestCondOblique\",\n",
    "    \"SampleProjection\", \"ApplyProjection\",\n",
    "    \"Bucket Allocation & Initialization=0\",\n",
    "    \"Filling & Finalizing the Buckets\", \"SortFeature\", \"ScanSplits\",\n",
    "    \"Post-processing after Training all Trees\",\n",
    "    \"EvaluateProjection\",\n",
    "    \"FillExampleBucketSet (next 3 calls)\",\n",
    "]\n",
    "\n",
    "ORDER_HISTOGRAM = [\n",
    "    \"Selecting Bootstrapped Samples\",\n",
    "    \"Initialization of FindBestCondOblique\",\n",
    "    \"SampleProjection\", \"ApplyProjection\",\n",
    "    \"Initializing Histogram Bins\",\n",
    "    \"Setting Split Distributions\",\n",
    "    \"Looping over samples\",\n",
    "    \"Looping over splits\",\n",
    "    \"Finding best threshold (Computing Entropies)\",\n",
    "    \"Post-processing after Training all Trees\",\n",
    "]\n",
    "\n",
    "RENAMES = {\n",
    "    \"Post-processing after Train\": \"Post-processing after Training all Trees\",\n",
    "    \"FillExampleBucketSet (calls 3 above)\": \"FillExampleBucketSet (next 3 calls)\",\n",
    "}\n",
    "\n",
    "TRAIN_RX = re.compile(r\"Training wall-time:\\s*([0-9.eE+-]+)s\")\n",
    "BOOT_TAG = \"Selecting Bootstrapped Samples\"\n",
    "DEPTH_TAG = \"Depth \"\n",
    "TOOK_TAG = \" took:\"\n",
    "STRIP_SET = \" \\t-\"\n",
    "\n",
    "# ============================================================================\n",
    "# PARSING FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def fast_parse_tree_depth(log: str, split_type: str = \"Exact\") -> pd.DataFrame:\n",
    "    \"\"\"Parse timing data from YDF log output.\"\"\"\n",
    "    def _num(tok: str) -> float:\n",
    "        tok = tok.rstrip()\n",
    "        if tok.endswith('s'):\n",
    "            tok = tok[:-1]\n",
    "        return float(tok)\n",
    "    \n",
    "    rows: list[tuple[int, int, str, float]] = []\n",
    "    node_counts: defaultdict[tuple[int, int], int] = defaultdict(int)\n",
    "\n",
    "    ORDER = ORDER_HISTOGRAM if split_type in [\"Random\", \"Equal Width\", \"Histogram\"] else ORDER_EXACT\n",
    "\n",
    "    cur_tree = -1\n",
    "    cur_depth: int | None = None\n",
    "\n",
    "    for line in log.splitlines():\n",
    "        # New tree (depth 0)\n",
    "        if BOOT_TAG in line:\n",
    "            cur_tree += 1\n",
    "            node_counts[(cur_tree, 0)] += 1\n",
    "            rows.append((cur_tree, 0, ORDER[0], _num(line.rsplit(maxsplit=1)[-1])))\n",
    "            cur_depth = None\n",
    "            continue\n",
    "\n",
    "        # Depth header\n",
    "        if line.lstrip().startswith(DEPTH_TAG):\n",
    "            cur_depth = int(line.lstrip()[len(DEPTH_TAG):].split()[0])\n",
    "            node_counts[(cur_tree, cur_depth)] += 1\n",
    "            continue\n",
    "\n",
    "        # Skip lines until at least one tree seen\n",
    "        if cur_tree < 0 or TOOK_TAG not in line:\n",
    "            continue\n",
    "\n",
    "        # Timing line\n",
    "        name_part, _, rest = line.partition(TOOK_TAG)\n",
    "        time_s = _num(rest.split()[0])\n",
    "\n",
    "        clean = name_part.lstrip(STRIP_SET).rstrip()\n",
    "        clean = RENAMES.get(clean, clean)\n",
    "\n",
    "        rows.append((cur_tree, cur_depth, clean, time_s))\n",
    "\n",
    "    if not rows:\n",
    "        raise ValueError(\"No timing lines parsed\")\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\"tree\", \"depth\", \"function\", \"time_s\"])\n",
    "\n",
    "    wide = (\n",
    "        df.pivot_table(index=[\"tree\", \"depth\"],\n",
    "                       columns=\"function\",\n",
    "                       values=\"time_s\",\n",
    "                       aggfunc=\"sum\",\n",
    "                       fill_value=0.0)\n",
    "          .reindex(columns=ORDER, fill_value=0.0)\n",
    "          .reset_index()\n",
    "    )\n",
    "\n",
    "    # Merge the node counts\n",
    "    counts_df = pd.DataFrame(\n",
    "        [(t, d, c) for (t, d), c in node_counts.items()],\n",
    "        columns=[\"tree\", \"depth\", \"nodes\"]\n",
    "    )\n",
    "    wide = wide.merge(counts_df, on=[\"tree\", \"depth\"], how=\"left\")\n",
    "    wide[\"nodes\"] = wide[\"nodes\"].fillna(0).astype(int)\n",
    "\n",
    "    cols = [\"tree\", \"depth\", \"nodes\"] + ORDER\n",
    "    return wide[cols]\n",
    "\n",
    "def write_csv(table: pd.DataFrame, params: Dict[str, Any], path: str):\n",
    "    \"\"\"Write timing table left-aligned, params block to the right (after 2 blanks).\"\"\"\n",
    "    p_df = pd.DataFrame(list(params.items()), columns=[\"Parameter\", \"Value\"])\n",
    "\n",
    "    n_rows = max(len(table), len(p_df))\n",
    "    tbl = table.reindex(range(n_rows)).fillna(\"\")\n",
    "    p_df = p_df.reindex(range(n_rows)).fillna(\"\")\n",
    "    gap = pd.DataFrame({\"\": [\"\"] * n_rows, \"  \": [\"\"] * n_rows})\n",
    "\n",
    "    pd.concat([tbl, gap, p_df], axis=1).to_csv(\n",
    "        path, index=False, quoting=csv.QUOTE_MINIMAL\n",
    "    )\n",
    "\n",
    "def extract_wall_time(log: str) -> str:\n",
    "    \"\"\"Extract wall time from log for filename.\"\"\"\n",
    "    match = TRAIN_RX.search(log)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        import time\n",
    "        return f\"unknown_{int(time.time())}\"\n",
    "\n",
    "def strip_ansi_codes(text: str) -> str:\n",
    "    \"\"\"Remove ANSI escape codes from text.\"\"\"\n",
    "    return re.sub(r'\\x1B\\[[0-?]*[ -/]*[@-~]', '', text)\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN INTERFACE\n",
    "# ============================================================================\n",
    "\n",
    "def parse_log_file(log_path: str, \n",
    "                   output_dir: str = None,\n",
    "                   split_type: str = \"Exact\",\n",
    "                   custom_params: Dict[str, Any] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Parse a YDF log file and optionally save to CSV.\n",
    "    \n",
    "    Args:\n",
    "        log_path: Path to the log file\n",
    "        output_dir: Directory to save CSV (if None, no CSV is saved)\n",
    "        split_type: Type of split used (\"Exact\", \"Random\", \"Equal Width\", \"Histogram\")\n",
    "        custom_params: Custom parameters to include in CSV\n",
    "    \n",
    "    Returns:\n",
    "        Parsed timing DataFrame\n",
    "    \"\"\"\n",
    "    print(f\"üìñ Reading log file: {log_path}\")\n",
    "    \n",
    "    with open(log_path, 'r') as f:\n",
    "        log_content = f.read()\n",
    "    \n",
    "    print(f\"üìù Log file size: {len(log_content)} characters\")\n",
    "    \n",
    "    # Strip ANSI codes\n",
    "    clean_log = strip_ansi_codes(log_content)\n",
    "    \n",
    "    # Parse the log\n",
    "    print(f\"‚öôÔ∏è  Parsing with split_type: {split_type}\")\n",
    "    table = fast_parse_tree_depth(clean_log, split_type)\n",
    "    \n",
    "    print(f\"‚úÖ Parsed {len(table)} rows\")\n",
    "    print(f\"   Trees: {table['tree'].nunique()}\")\n",
    "    print(f\"   Max depth: {table['depth'].max()}\")\n",
    "    \n",
    "    # Extract wall time\n",
    "    wall_time = extract_wall_time(clean_log)\n",
    "    print(f\"‚è±Ô∏è  Wall time: {wall_time}s\")\n",
    "    \n",
    "    # Save CSV if output directory provided\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        csv_path = os.path.join(output_dir, f\"{wall_time}.csv\")\n",
    "        \n",
    "        # Default parameters\n",
    "        params = {\n",
    "            \"wall_time\": wall_time,\n",
    "            \"split_type\": split_type,\n",
    "            \"log_source\": os.path.basename(log_path),\n",
    "        }\n",
    "        \n",
    "        # Add custom parameters if provided\n",
    "        if custom_params:\n",
    "            params.update(custom_params)\n",
    "        \n",
    "        write_csv(table, params, csv_path)\n",
    "        print(f\"üíæ CSV saved to: {csv_path}\")\n",
    "    \n",
    "    return table\n",
    "\n",
    "def parse_log_string(log_content: str,\n",
    "                     output_path: str = None,\n",
    "                     split_type: str = \"Exact\",\n",
    "                     custom_params: Dict[str, Any] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Parse a YDF log from string content.\n",
    "    \n",
    "    Args:\n",
    "        log_content: Raw log content as string\n",
    "        output_path: Full path to save CSV (if None, no CSV is saved)\n",
    "        split_type: Type of split used (\"Exact\", \"Random\", \"Equal Width\", \"Histogram\")\n",
    "        custom_params: Custom parameters to include in CSV\n",
    "    \n",
    "    Returns:\n",
    "        Parsed timing DataFrame\n",
    "    \"\"\"\n",
    "    print(f\"üìù Processing log content ({len(log_content)} characters)\")\n",
    "    \n",
    "    # Strip ANSI codes\n",
    "    clean_log = strip_ansi_codes(log_content)\n",
    "    \n",
    "    # Parse the log\n",
    "    print(f\"‚öôÔ∏è  Parsing with split_type: {split_type}\")\n",
    "    table = fast_parse_tree_depth(clean_log, split_type)\n",
    "    \n",
    "    print(f\"‚úÖ Parsed {len(table)} rows\")\n",
    "    print(f\"   Trees: {table['tree'].nunique()}\")\n",
    "    print(f\"   Max depth: {table['depth'].max()}\")\n",
    "    \n",
    "    # Extract wall time\n",
    "    wall_time = extract_wall_time(clean_log)\n",
    "    print(f\"‚è±Ô∏è  Wall time: {wall_time}s\")\n",
    "    \n",
    "    # Save CSV if output path provided\n",
    "    if output_path:\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        \n",
    "        # Default parameters\n",
    "        params = {\n",
    "            \"wall_time\": wall_time,\n",
    "            \"split_type\": split_type,\n",
    "            \"log_source\": \"string_input\",\n",
    "        }\n",
    "        \n",
    "        # Add custom parameters if provided\n",
    "        if custom_params:\n",
    "            params.update(custom_params)\n",
    "        \n",
    "        write_csv(table, params, output_path)\n",
    "        print(f\"üíæ CSV saved to: {output_path}\")\n",
    "    \n",
    "    return table\n",
    "\n",
    "# ============================================================================\n",
    "# EXAMPLE USAGE\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example 1: Parse a single log file\n",
    "    log_file = \"path/to/your/logfile.log\"\n",
    "    output_directory = \"parsed_results\"\n",
    "    \n",
    "    # Custom parameters to include in the CSV\n",
    "    my_params = {\n",
    "        \"rows\": 4096,\n",
    "        \"cols\": 4096,\n",
    "        \"num_trees\": 5,\n",
    "        \"tree_depth\": -1,\n",
    "        \"num_threads\": 1,\n",
    "        \"experiment_name\": \"my_experiment\",\n",
    "        \"cpu_model\": \"Intel_i7_something\"\n",
    "    }\n",
    "    \n",
    "    # Parse and save\n",
    "    df = parse_log_file(log_file, output_directory, \"Exact\", my_params)\n",
    "    \n",
    "    # Example 2: Parse from string (useful in notebooks)\n",
    "    # log_string = \"\"\"\n",
    "    # Your log content here...\n",
    "    # \"\"\"\n",
    "    # # df = parse_log_string(log_string, \"output.csv\", \"Exact\", my_params)\n",
    "    \n",
    "    # print(\"Ready to parse! Uncomment the examples above or use the functions directly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b1813a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
